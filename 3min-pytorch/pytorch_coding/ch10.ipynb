{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "ch10",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "### 카트폴 게임 마스터하기"
      ],
      "metadata": {
        "id": "iP7xB0LOMLVA"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UhlW_jJrMC2I"
      },
      "outputs": [],
      "source": [
        "import gym\n",
        "import random\n",
        "import math\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import torch.nn.functional as F\n",
        "from collections import deque\n",
        "import matplotlib.pyplot as plt\n",
        "from pyvirtualdisplay import Display"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "!apt-get install -y xvfb x11-utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JeZ5PCB2i6dc",
        "outputId": "19d12bec-7c10-4cbd-e5e0-c3099dd0a34b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "The following additional packages will be installed:\n",
            "  libxxf86dga1\n",
            "Suggested packages:\n",
            "  mesa-utils\n",
            "The following NEW packages will be installed:\n",
            "  libxxf86dga1 x11-utils xvfb\n",
            "0 upgraded, 3 newly installed, 0 to remove and 41 not upgraded.\n",
            "Need to get 993 kB of archives.\n",
            "After this operation, 2,982 kB of additional disk space will be used.\n",
            "Get:1 http://archive.ubuntu.com/ubuntu bionic/main amd64 libxxf86dga1 amd64 2:1.1.4-1 [13.7 kB]\n",
            "Get:2 http://archive.ubuntu.com/ubuntu bionic/main amd64 x11-utils amd64 7.7+3build1 [196 kB]\n",
            "Get:3 http://archive.ubuntu.com/ubuntu bionic-updates/universe amd64 xvfb amd64 2:1.19.6-1ubuntu4.10 [784 kB]\n",
            "Fetched 993 kB in 1s (1,519 kB/s)\n",
            "Selecting previously unselected package libxxf86dga1:amd64.\n",
            "(Reading database ... 155514 files and directories currently installed.)\n",
            "Preparing to unpack .../libxxf86dga1_2%3a1.1.4-1_amd64.deb ...\n",
            "Unpacking libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Selecting previously unselected package x11-utils.\n",
            "Preparing to unpack .../x11-utils_7.7+3build1_amd64.deb ...\n",
            "Unpacking x11-utils (7.7+3build1) ...\n",
            "Selecting previously unselected package xvfb.\n",
            "Preparing to unpack .../xvfb_2%3a1.19.6-1ubuntu4.10_amd64.deb ...\n",
            "Unpacking xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up xvfb (2:1.19.6-1ubuntu4.10) ...\n",
            "Setting up libxxf86dga1:amd64 (2:1.1.4-1) ...\n",
            "Setting up x11-utils (7.7+3build1) ...\n",
            "Processing triggers for man-db (2.8.3-2ubuntu0.1) ...\n",
            "Processing triggers for libc-bin (2.27-3ubuntu1.3) ...\n",
            "/sbin/ldconfig.real: /usr/local/lib/python3.7/dist-packages/ideep4py/lib/libmkldnn.so.0 is not a symbolic link\n",
            "\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "display = Display(visible=False, size=(400, 300))\n",
        "display.start()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TtAz5y7IkUw3",
        "outputId": "3ba6f9f6-2c83-4ef4-9ea6-a45eba4e3124"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<Display cmd_param=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] cmd=['Xvfb', '-br', '-nolisten', 'tcp', '-screen', '0', '400x300x24', ':1001'] oserror=None return_code=None stdout=\"None\" stderr=\"None\" timeout_happened=False>"
            ]
          },
          "metadata": {},
          "execution_count": 30
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "EPISODES = 50       # 에피소드 반복 횟수\n",
        "EPS_START = 0.9     # 학습 시작 시 에이전트가 무작위로 행동할 확률\n",
        "EPS_END = 0.05      # 학습 막바지에 에이전트가 무작위로 행동할 확률\n",
        "EPS_DECAY = 200     # 학습 진행 시 에이전트가 무작위로 행동할 확률을 감소시키는 값\n",
        "GAMMA = 0.8         # 할인계수\n",
        "LR = 0.001\n",
        "BATCH_SIZE = 64"
      ],
      "metadata": {
        "id": "fePfu2qyNhVn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class DQNAgent:\n",
        "    def __init__(self):\n",
        "        self.model = nn.Sequential(\n",
        "            nn.Linear(4, 256),  # input: 카트 (위치, 속도), 막대기 (각도, 속도)\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(256, 2)   # output: 왼쪽 or 오른쪽\n",
        "        )\n",
        "        self.optimizer = optim.Adam(self.model.parameters(), LR)\n",
        "        self.steps_done = 0   # 학습 반복할 때마다 증가하는 변수\n",
        "        self.memory = deque(maxlen=10000)   # self.memory = [(상태, 행동, 보상, 다음 상태) ......]\n",
        "    \n",
        "    def memorize(self, state, action, reward, next_state):\n",
        "        self.memory.append((state,\n",
        "                            action, torch.FloatTensor([reward]),\n",
        "                            torch.FloatTensor([next_state])))\n",
        "    \n",
        "    def act(self, state):\n",
        "        eps_threshold = EPS_END + (EPS_START - EPS_END) * math.exp(-1. * self.steps_done / EPS_DECAY)\n",
        "        self.steps_done += 1\n",
        "        if random.random() > eps_threshold:\n",
        "            return self.model(state).data.max(1)[1].view(1, 1)\n",
        "        else:\n",
        "            return torch.LongTensor([[random.randrange(2)]])\n",
        "\n",
        "    def learn(self):\n",
        "        if len(self.memory) < BATCH_SIZE:\n",
        "            return\n",
        "        batch = random.sample(self.memory, BATCH_SIZE)  # 경험들 중 무작위로 가져옴 -> 경험샘플의 상관성 줄임\n",
        "        states, actions, rewards, next_states = zip(*batch)\n",
        "\n",
        "        states = torch.cat(states)\n",
        "        actions = torch.cat(actions)\n",
        "        rewards = torch.cat(rewards)\n",
        "        next_states = torch.cat(next_states)\n",
        "\n",
        "        current_q = self.model(states)\n",
        "\n",
        "        max_next_q = self.model(next_states).detach().max(1)[0]\n",
        "        expected_q = rewards + (GAMMA * max_next_q)\n",
        "\n",
        "        loss = F.mse_loss(current_q.squeeze(), expected_q)\n",
        "        self.optimizer.zero_grad()\n",
        "        loss.backward()\n",
        "        self.optimizer.step()"
      ],
      "metadata": {
        "id": "0MZOKxvoNsQU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "$\\epsilon \\;\\; threshold = \\epsilon_{end} + ( \\epsilon_{start} - \\epsilon_{end})  * e^{-step/\\epsilon_{decay}}$"
      ],
      "metadata": {
        "id": "jwUfA2N-dVfK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "env = gym.make(\"CartPole-v0\")\n",
        "\n",
        "agent = DQNAgent()\n",
        "score_history = []\n",
        "\n",
        "for e in range(1, EPISODES+1):\n",
        "    state = env.reset()\n",
        "    steps = 0\n",
        "\n",
        "    while True:\n",
        "        env.render()\n",
        "        state = torch.FloatTensor([state])\n",
        "        action = agent.act(state)\n",
        "        next_state, reward, done, _ = env.step(action.item())\n",
        "\n",
        "        if done:\n",
        "            reward = -1\n",
        "        \n",
        "        agent.memorize(state, action, reward, next_state)\n",
        "        agent.learn()\n",
        "\n",
        "        state = next_state\n",
        "        steps += 1\n",
        "\n",
        "        if done:\n",
        "            print(f\"에피소드: {e}, 점수: {steps}\")\n",
        "            score_history.append(steps)\n",
        "            break"
      ],
      "metadata": {
        "id": "HtefLXCmhpCA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        ""
      ],
      "metadata": {
        "id": "4_XmhETbkbgS"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}